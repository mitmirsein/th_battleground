import asyncio
import os
import subprocess
import json
from pathlib import Path
from typing import Optional, Dict, Any, List
from playwright.async_api import async_playwright
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€ (utils ì ‘ê·¼ì„ ìœ„í•´)
sys.path.append(str(Path(__file__).parent.parent))

try:
    from utils.local_pdf_processor import process_pdf, tiktoken_len
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    print("âš ï¸ PDF processing utilities not available")


class AgentBrowser:
    """Agent-Browser CLI wrapper for AI-friendly web automation"""
    
    def __init__(self, session: str = "librarian"):
        self.session = session
        
    def _run(self, cmd: List[str], json_output: bool = False) -> Dict:
        """Execute agent-browser command"""
        full_cmd = ["agent-browser"] + cmd
        if json_output:
            full_cmd.append("--json")
        full_cmd.extend(["--session", self.session])
        
        result = subprocess.run(full_cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            return {"success": False, "error": result.stderr}
        
        if json_output:
            try:
                return json.loads(result.stdout)
            except:
                return {"success": False, "raw": result.stdout}
        
        return {"success": True, "stdout": result.stdout}
    
    def open(self, url: str): return self._run(["open", url])
    def wait(self, ms: int): return self._run(["wait", str(ms)])
    def snapshot(self): return self._run(["snapshot", "-i"], json_output=True)
    def get_text(self, ref: str): return self._run(["get", "text", ref], json_output=True)
    def click(self, ref: str): return self._run(["click", ref])
    def screenshot(self, path: str): return self._run(["screenshot", path])
    def close(self): return self._run(["close"])


class LibrarianAgent:
    """
    ì •ë³´ìˆ˜ì§‘ê´€ (Librarian)
    - ì™¸ë¶€ ì›¹ URL ì½˜í…ì¸  ì¶”ì¶œ
    - ë¡œì»¬ PDF íŒŒì¼ ì •ì œ ë° í…ìŠ¤íŠ¸í™”
    - agent-browserë¥¼ í†µí•œ AI ì¹œí™”ì  ìŠ¤í¬ë˜í•‘
    """
    
    def __init__(self, persona_path: Optional[str] = None):
        self.persona_path = persona_path
        self.name = "Librarian"
        self.browser = AgentBrowser("librarian_session")
        
    async def collect_web(self, url: str) -> Dict[str, Any]:
        """ì›¹ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ Playwright ë°©ì‹)"""
        print(f"ğŸŒ [{self.name}] ì›¹ ìˆ˜ì§‘ ì‹œì‘: {url}")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                await page.goto(url, wait_until="networkidle")
                title = await page.title()
                content = await page.evaluate("document.body.innerText")
                
                print(f"âœ… [{self.name}] ìˆ˜ì§‘ ì™„ë£Œ: {title}")
                return {
                    "source": url,
                    "title": title,
                    "content": content,
                    "type": "web"
                }
            except Exception as e:
                print(f"âŒ [{self.name}] ì›¹ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                return {"error": str(e)}
            finally:
                await browser.close()

    def collect_web_agent(self, url: str, wait_ms: int = 5000) -> Dict[str, Any]:
        """
        agent-browserë¥¼ ì‚¬ìš©í•œ ì›¹ ìˆ˜ì§‘ (ë™ê¸°ì‹, AI ì¹œí™”ì )
        Returns: snapshot with refs for further interaction
        """
        print(f"ğŸ¤– [{self.name}] Agent-Browser ìˆ˜ì§‘ ì‹œì‘: {url}")
        
        self.browser.open(url)
        self.browser.wait(wait_ms)
        
        snapshot = self.browser.snapshot()
        
        if snapshot.get("success"):
            refs = snapshot.get("data", {}).get("refs", {})
            print(f"âœ… [{self.name}] ë°œê²¬ëœ ìš”ì†Œ: {len(refs)}ê°œ")
            return {
                "source": url,
                "snapshot": snapshot.get("data", {}).get("snapshot", ""),
                "refs": refs,
                "type": "agent-browser"
            }
        else:
            print(f"âŒ [{self.name}] ìŠ¤ëƒ…ìƒ· ì‹¤íŒ¨: {snapshot.get('error')}")
            return {"error": snapshot.get("error", "Unknown error")}

    def scrape_journal_toc(self, journal: str, band: int, heft: int) -> List[Dict]:
        """
        ì €ë„ ëª©ì°¨ ìŠ¤í¬ë˜í•‘ (ì €ë„ë³„ ë¡œì§)
        
        Args:
            journal: ì €ë„ ì•½ì¹­ (kud, evth, znw)
            band: ê¶Œ ë²ˆí˜¸
            heft: í˜¸ ë²ˆí˜¸
        
        Returns:
            List of article dicts: [{title, author, pages}, ...]
        """
        urls = {
            "kud": f"https://www.vr-elibrary.de/toc/kud/{band}/{heft}",
            "evth": f"https://www.degruyter.com/journal/key/evth/volume/{band}/issue/{heft}/html",
            "znw": f"https://www.degruyter.com/journal/key/znw/volume/{band}/issue/{heft}/html"
        }
        
        url = urls.get(journal.lower())
        if not url:
            return [{"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì €ë„: {journal}"}]
        
        print(f"ğŸ“š [{self.name}] ì €ë„ ìŠ¤í¬ë˜í•‘: {journal.upper()} {band}/{heft}")
        
        result = self.collect_web_agent(url, wait_ms=8000)
        
        if "error" in result:
            return [result]
        
        # Extract articles from snapshot (ì €ë„ë³„ íŒŒì‹± ë¡œì§ í•„ìš”)
        articles = self._parse_journal_snapshot(result, journal)
        
        print(f"âœ… [{self.name}] ë°œê²¬ëœ ë…¼ë¬¸: {len(articles)}í¸")
        return articles
    
    def _parse_journal_snapshot(self, result: Dict, journal: str) -> List[Dict]:
        """ìŠ¤ëƒ…ìƒ·ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ (ì €ë„ë³„ ë¡œì§)"""
        refs = result.get("refs", {})
        articles = []
        
        for ref_id, elem in refs.items():
            role = elem.get("role", "")
            name = elem.get("name", "")
            
            # ë…¼ë¬¸ ì œëª© íŒ¨í„´ (heading level 3 or link with article pattern)
            if role in ["heading", "link"] and len(name) > 20:
                # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: 20ì ì´ìƒì˜ heading/linkëŠ” ë…¼ë¬¸ ì œëª©ì¼ ê°€ëŠ¥ì„±
                articles.append({
                    "title": name,
                    "ref": ref_id,
                    "journal": journal.upper()
                })
        
        return articles

    def close_browser(self):
        """ë¸Œë¼ìš°ì € ì„¸ì…˜ ì¢…ë£Œ"""
        self.browser.close()

    def collect_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """ë¡œì»¬ PDF ê°€ê³µ (utils.local_pdf_processor í™œìš©)"""
        print(f"ğŸ“„ [{self.name}] PDF ê°€ê³µ ì‹œì‘: {pdf_path}")
        
        if not os.path.exists(pdf_path):
            return {"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}"}
            
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=560,
            separators=["\n\n\n", "\n\n", "\n", ". ", "! ", "? ", "; ", " ", ""],
            length_function=tiktoken_len,
        )
        
        try:
            chunks = process_pdf(pdf_path, text_splitter)
            full_text = "\n\n".join([c['text'] for c in chunks])
            
            print(f"âœ… [{self.name}] PDF ê°€ê³µ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return {
                "source": pdf_path,
                "title": Path(pdf_path).name,
                "chunks": chunks,
                "full_text": full_text,
                "type": "pdf"
            }
        except Exception as e:
            print(f"âŒ [{self.name}] PDF ê°€ê³µ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ARC Secretariat - Librarian Agent")
    parser.add_argument("--url", type=str, help="Collect content from a web URL")
    parser.add_argument("--pdf", type=str, help="Process a local PDF file")
    parser.add_argument("--journal", type=str, help="Scrape journal TOC (kud, evth, znw)")
    parser.add_argument("--band", type=int, help="Journal volume number")
    parser.add_argument("--heft", type=int, help="Journal issue number")
    parser.add_argument("--agent", action="store_true", help="Use agent-browser instead of Playwright")
    
    args = parser.parse_args()
    
    async def run_cli():
        lib = LibrarianAgent()
        
        if args.journal and args.band and args.heft:
            # ì €ë„ ìŠ¤í¬ë˜í•‘
            result = lib.scrape_journal_toc(args.journal, args.band, args.heft)
            print(json.dumps(result, ensure_ascii=False, indent=2))
            lib.close_browser()
        elif args.url:
            if args.agent:
                result = lib.collect_web_agent(args.url)
                lib.close_browser()
            else:
                result = await lib.collect_web(args.url)
            print(json.dumps(result, ensure_ascii=False, indent=2))
        elif args.pdf:
            result = lib.collect_pdf(args.pdf)
            if "chunks" in result:
                del result["chunks"]
            print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            parser.print_help()
    
    if args.url or args.pdf or (args.journal and args.band and args.heft):
        asyncio.run(run_cli())

