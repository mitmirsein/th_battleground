#!/usr/bin/env python3
"""
Google Scholar Labs ì‹œë§¨í‹± ê²€ìƒ‰ ì—ì´ì „íŠ¸ (v2)
- ì„¸ì…˜ ì œí•œ ëŒ€ì‘: ì§ˆë¬¸ 3ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
- ëˆ„ì  ì €ì¥: ë™ì¼ íŒŒì¼ì— ê²°ê³¼ ëˆ„ì 
"""

import asyncio
import json
import re
import sys
from datetime import datetime
from pathlib import Path

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout


class ScholarLabsAgent:
    """Google Scholar Labs ì‹œë§¨í‹± ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
    
    SCHOLAR_LABS_URL = "https://scholar.google.com/scholar_labs/search?hl=ko"
    RESULTS_DIR = Path(__file__).parent / "results"
    PROFILES_DIR = Path(__file__).parent / ".profiles"
    BATCH_SIZE = 5  # ì„¸ì…˜ ì œí•œ ëŒ€ì‘: í•œ ì„¸ì…˜ë‹¹ ìµœëŒ€ ì§ˆë¬¸ ìˆ˜
    
    def __init__(self, profile: str = "default", job: str = None):
        self.profile = profile
        self.results_dir = self.RESULTS_DIR
        self.results_dir.mkdir(exist_ok=True)
        self.profiles_dir = self.PROFILES_DIR
        self.profiles_dir.mkdir(exist_ok=True)
        
        # ì‘ì—…(Job) ì´ë¦„ ì„¤ì • (ê¸°ë³¸ê°’: íƒ€ì„ìŠ¤íƒ¬í”„)
        if job is None:
            job = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.job = job
        
        # ì‘ì—…ë³„ ìƒíƒœ íŒŒì¼ ë° ê²°ê³¼ íŒŒì¼
        self.state_file = self.results_dir / f".state_{job}.json"
        self.output_file = self.results_dir / f"{job}.md"
        
        # í”„ë¡œí•„ë³„ ë¸Œë¼ìš°ì € ë°ì´í„° ë””ë ‰í† ë¦¬
        self.user_data_dir = self.profiles_dir / profile
        self.user_data_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ í”„ë¡œí•„: {profile}")
        print(f"ğŸ“‹ ì‘ì—…ëª…: {job}")
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {self.output_file}")
    
    def parse_queries(self, raw_input: str) -> list[str]:
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê²€ìƒ‰ ì§ˆë¬¸ ëª©ë¡ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        lines = raw_input.strip().split('\n')
        queries = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Q1:, Q2: í˜•ì‹ ë˜ëŠ” 1., 2. í˜•ì‹ ì œê±°
            cleaned = re.sub(r'^(Q?\d+[\.:]\s*)', '', line, flags=re.IGNORECASE)
            cleaned = cleaned.strip()
            
            if cleaned:
                queries.append(cleaned)
        
        return queries
    
    def load_state(self) -> dict:
        """ì´ì „ ê²€ìƒ‰ ìƒíƒœ ë¡œë“œ"""
        if self.state_file.exists():
            return json.loads(self.state_file.read_text(encoding='utf-8'))
        return {"completed_queries": [], "results": []}
    
    def save_state(self, state: dict):
        """ê²€ìƒ‰ ìƒíƒœ ì €ì¥"""
        self.state_file.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding='utf-8')
    
    def clear_state(self):
        """ê²€ìƒ‰ ìƒíƒœ ì´ˆê¸°í™”"""
        if self.state_file.exists():
            self.state_file.unlink()
    
    async def search_scholar_labs_fast(self, page, query: str) -> bool:
        """ê²€ìƒ‰ ì…ë ¥ í›„ ê²°ê³¼ê°€ ì•ˆì •í™”ë  ë•Œê¹Œì§€ ì§€ëŠ¥í˜• ëŒ€ê¸° (AI ë¶„ì„ ì‹œê°„ ê³ ë ¤)
        
        Returns:
            True if results appeared and stabilized, False otherwise
        """
        try:
            # ê²€ìƒ‰ì°½ ì°¾ê¸°
            search_box = None
            selectors = ['textarea#gs_as_i_t', 'textarea[name="q"]', 'textarea', 'input[type="text"]']
            
            for selector in selectors:
                try:
                    search_box = await page.wait_for_selector(selector, timeout=5000)
                    if search_box:
                        break
                except:
                    continue
            
            if not search_box:
                return False
            
            await search_box.fill('')
            await search_box.fill(query)
            await search_box.press('Enter')
            print(f"    âœ“ ê²€ìƒ‰ ì œì¶œ")
            
            # ì§€ëŠ¥í˜• ëŒ€ê¸° ë¡œì§: ê²°ê³¼ ê°œìˆ˜ ì•ˆì •í™” ê°ì§€
            print(f"    â³ AI ë¶„ì„ ë° ê²°ê³¼ ìƒì„± ëŒ€ê¸° ì¤‘ (ìµœëŒ€ 90ì´ˆ)...")
            
            stable_ticks = 0
            last_count = 0
            max_wait = 90
            
            for i in range(max_wait):
                await asyncio.sleep(1)
                
                # í˜„ì¬ ìœ íš¨ ê²°ê³¼ ê°œìˆ˜ í™•ì¸
                all_links = await page.query_selector_all('a[id]')
                current_count = 0
                for link in all_links:
                    try:
                        lid = await link.get_attribute('id')
                        if lid and not lid.startswith('gs_'):
                            current_count += 1
                    except:
                        continue
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (3ì´ˆë§ˆë‹¤ ë³€ê²½ ì‹œì—ë§Œ)
                if i > 0 and i % 3 == 0:
                    # 'í‰ê°€ë¨' í…ìŠ¤íŠ¸ í™•ì¸ (ë³´ì¡° ì§€í‘œ)
                    try:
                        body_text = await page.inner_text('body')
                        status = "ë¶„ì„ ì¤‘"
                        if 'í‰ê°€ë¨' in body_text or 'Evaluated' in body_text:
                            status = "í‰ê°€ë¨"
                        print(f"       [{i}s] ë…¼ë¬¸: {current_count}ê°œ ({status})...")
                    except:
                        pass

                # ì•ˆì •í™” ì²´í¬ ë¡œì§
                if current_count > 0:
                    if current_count == last_count:
                        stable_ticks += 1
                    else:
                        stable_ticks = 0 # ë³€í™”ê°€ ìˆìœ¼ë©´ ë¦¬ì…‹
                    
                    # 5ì´ˆ ì—°ì† ë³€í™” ì—†ê³ , ìµœì†Œ 1ê°œ ì´ìƒì´ë©´ ì™„ë£Œ
                    if stable_ticks >= 5 and current_count >= 1:
                        print(f"    âœ“ ê²°ê³¼ ì•ˆì •í™” ì™„ë£Œ ({current_count}ê°œ ìˆ˜ì§‘, {i}ì´ˆ ì†Œìš”)")
                        return True
                
                last_count = current_count
            
            if last_count > 0:
                print(f"    âš  ëŒ€ê¸° ì‹œê°„ ì¢…ë£Œ, {last_count}ê°œ ê²°ê³¼ë¡œ ì§„í–‰")
                return True
            
            print(f"    âš  ê²°ê³¼ ì—†ìŒ (Timeout)")
            return False
            
        except Exception as e:
            print(f"    âŒ ê²€ìƒ‰ ì œì¶œ ì˜¤ë¥˜: {e}")
            return False
    
    async def extract_citations_from_page(self, page, max_results: int = 10) -> list[dict]:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ì¸ìš© ì •ë³´ë¥¼ ì¶”ì¶œ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
        # ì•ˆì •í™” ëŒ€ê¸°
        await asyncio.sleep(5)
        results = await self._parse_labs_results(page, max_results)
        return results
    
    async def search_scholar_labs(self, page, query: str, max_results: int = 10, is_first_in_batch: bool = False) -> list[dict]:
        """Google Scholar Labsì—ì„œ ì‹œë§¨í‹± ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ë ˆê±°ì‹œ í˜¸í™˜)"""
        MAX_RETRY = 2
        results = []
        
        try:
            search_box = None
            selectors = ['textarea#gs_as_i_t', 'textarea[name="q"]', 'textarea', 'input[type="text"]']
            
            for selector in selectors:
                try:
                    search_box = await page.wait_for_selector(selector, timeout=5000)
                    if search_box:
                        print(f"    ğŸ“ ê²€ìƒ‰ì°½ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not search_box:
                print("    âŒ ê²€ìƒ‰ì°½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return results
            
            await search_box.fill('')
            await search_box.fill(query)
            await search_box.press('Enter')
            
            # ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶• (30/15ì´ˆ)
            wait_time = 30 if is_first_in_batch else 15
            print(f"    â³ ê²°ê³¼ ë¡œë”© ì¤‘... (ì•½ {wait_time}ì´ˆ)")
            await asyncio.sleep(wait_time)
            
            try:
                await page.wait_for_selector('text=í‰ê°€ë¨', timeout=30000)
                print("    âœ“ í‰ê°€ ì™„ë£Œ")
            except:
                print("    âš  í‰ê°€ í™•ì¸ ì‹¤íŒ¨, ê³„ì†...")
            
            await asyncio.sleep(5)
            
            for attempt in range(MAX_RETRY + 1):
                results = await self._parse_labs_results(page, max_results)
                if results:
                    break
                if attempt < MAX_RETRY:
                    print(f"    âš  ì¬ì‹œë„ {attempt + 1}/{MAX_RETRY}...")
                    await asyncio.sleep(10)
            
        except PlaywrightTimeout:
            print(f"  â± íƒ€ì„ì•„ì›ƒ: {query[:50]}...")
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return results
    
    async def _parse_labs_results(self, page, max_results: int) -> list[dict]:
        """Scholar Labs ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        results = []
        
        all_links = await page.query_selector_all('a[id]')
        
        paper_links = []
        for link in all_links:
            link_id = await link.get_attribute('id')
            href = await link.get_attribute('href')
            
            if link_id and href and not link_id.startswith('gs_'):
                try:
                    text = await link.inner_text()
                    if text and len(text) > 10:
                        paper_links.append({'element': link, 'id': link_id, 'href': href, 'title': text})
                except:
                    continue
        
        for i, paper in enumerate(paper_links[:max_results]):
            try:
                result = {
                    'title': paper['title'].strip(),
                    'link': paper['href'],
                }
                
                parent = await paper['element'].evaluate_handle('el => el.parentElement.parentElement')
                parent_text = await parent.evaluate('el => el.innerText')
                lines = parent_text.split('\n')
                
                meta_line = ""
                snippet_lines = []
                
                for line in lines:
                    line = line.strip()
                    if not line or line == result['title']:
                        continue
                    if 'ì¸ìš©' in line and 'íšŒ' in line:
                        result['citations'] = line
                    elif re.search(r'\d{4}', line) and len(line) < 150:
                        if not meta_line:
                            meta_line = line
                    else:
                        if len(line) > 20:
                            snippet_lines.append(line)
                
                result['meta'] = meta_line
                result['snippet'] = ' '.join(snippet_lines[:3])
                
                # MLA ì¸ìš© ì¶”ì¶œ ì‹œë„
                mla_citation = await self._extract_mla_citation(page, paper['element'])
                if mla_citation:
                    result['mla_citation'] = mla_citation
                
                results.append(result)
                
            except Exception as e:
                continue
        
        return results
    
    async def _extract_mla_citation(self, page, paper_element) -> str:
        """ì¸ìš© ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ MLA í˜•ì‹ ì¸ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ë…¼ë¬¸ ìš”ì†Œì˜ ë¶€ëª¨ì—ì„œ 'ì¸ìš©' ë§í¬ ì°¾ê¸°
            parent = await paper_element.evaluate_handle('el => el.parentElement.parentElement.parentElement')
            
            # "ì¸ìš©" ë˜ëŠ” "ì €ì¥" í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬ ì°¾ê¸°
            cite_link = await parent.query_selector('a:has-text("ì¸ìš©")')
            
            if not cite_link:
                # ëŒ€ì•ˆ: í´ë˜ìŠ¤ë‚˜ ë‹¤ë¥¸ ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
                cite_link = await parent.query_selector('a[aria-label*="ì¸ìš©"]')
            
            if not cite_link:
                return ""
            
            # ì¸ìš© ë§í¬ í´ë¦­
            await cite_link.click()
            await asyncio.sleep(1.5)  # íŒì—… ë¡œë”© ëŒ€ê¸°
            
            # MLA í˜•ì‹ í…ìŠ¤íŠ¸ ì¶”ì¶œ (íŒì—…ì—ì„œ)
            # Scholarì˜ ì¸ìš© íŒì—…ì€ ë³´í†µ #gs_cit ë˜ëŠ” ìœ ì‚¬í•œ IDë¥¼ ê°€ì§
            mla_text = ""
            
            # ë°©ë²• 1: MLA íƒ­/ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°
            mla_tab = await page.query_selector('a:has-text("MLA")')
            if mla_tab:
                await mla_tab.click()
                await asyncio.sleep(0.5)
            
            # ë°©ë²• 2: ì¸ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ
            citation_box = await page.query_selector('#gs_citt')
            if citation_box:
                mla_text = await citation_box.inner_text()
            else:
                # ëŒ€ì•ˆ: íŒì—… ë‚´ ì²« ë²ˆì§¸ ì¸ìš© í…ìŠ¤íŠ¸
                citation_div = await page.query_selector('.gs_citr')
                if citation_div:
                    mla_text = await citation_div.inner_text()
            
            # íŒì—… ë‹«ê¸° (ESC í‚¤ ë˜ëŠ” ë‹«ê¸° ë²„íŠ¼)
            close_btn = await page.query_selector('#gs_cit-x')
            if close_btn:
                await close_btn.click()
            else:
                await page.keyboard.press('Escape')
            
            await asyncio.sleep(0.5)
            
            return mla_text.strip() if mla_text else ""
            
        except Exception as e:
            # íŒì—…ì´ ì—´ë ¤ìˆë‹¤ë©´ ë‹«ê¸° ì‹œë„
            try:
                await page.keyboard.press('Escape')
            except:
                pass
            return ""
    
    async def run_batch(self, queries: list[str], start_index: int) -> list[tuple[str, list[dict]]]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê²€ìƒ‰ ì‹¤í–‰ (íƒ­ ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬)"""
        batch_results = []
        
        async with async_playwright() as p:
            context = await p.chromium.launch_persistent_context(
                user_data_dir=str(self.user_data_dir),
                headless=False,
                args=['--disable-blink-features=AutomationControlled']
            )
            page = context.pages[0] if context.pages else await context.new_page()
            
            print(f"\nğŸŒ ë¸Œë¼ìš°ì € ì„¸ì…˜ ì‹œì‘ (ë³‘ë ¬ ëª¨ë“œ)")
            print(f"   í”„ë¡œí•„: {self.user_data_dir}")
            
            await page.goto(self.SCHOLAR_LABS_URL, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(3)
            
            # ë¡œê·¸ì¸ ì²´í¬
            if await self._check_login_required(page):
                print("\nâš ï¸  Google ë¡œê·¸ì¸ í•„ìš”!")
                print("    ë¡œê·¸ì¸ í›„ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                input()
                await page.goto(self.SCHOLAR_LABS_URL, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(3)
            
            # ì¼ì¼ í•œë„ ì²´í¬
            if await self._check_daily_limit(page):
                print("\nâŒ ì¼ì¼ í•œë„ ë„ë‹¬!")
                await context.close()
                return batch_results
            
            # ë³‘ë ¬ ì²˜ë¦¬ ë£¨í”„
            prev_tab = None
            prev_query = None
            extraction_task = None
            
            for i, query in enumerate(queries):
                q_num = start_index + i + 1
                print(f"\n[Q{q_num}] {query[:60]}{'...' if len(query) > 60 else ''}")
                
                # ì´ì „ íƒ­ì˜ ì¸ìš© ì¶”ì¶œì´ ì§„í–‰ ì¤‘ì´ë©´ ëŒ€ê¸°
                if extraction_task:
                    print(f"    â³ ì´ì „ ì§ˆë¬¸ ì¸ìš© ì¶”ì¶œ ëŒ€ê¸° ì¤‘...")
                    prev_results = await extraction_task
                    batch_results.append((prev_query, prev_results))
                    print(f"    âœ… ì´ì „ ì§ˆë¬¸: {len(prev_results)}ê°œ ì™„ë£Œ")
                    await prev_tab.close()
                    extraction_task = None
                
                # ìƒˆ íƒ­ì—ì„œ ê²€ìƒ‰ ì œì¶œ
                current_tab = await context.new_page()
                await current_tab.goto(self.SCHOLAR_LABS_URL, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(2)
                
                # 10ê°œ ê²°ê³¼ í‘œì‹œ í™•ì¸ê¹Œì§€ë§Œ ëŒ€ê¸°
                success = await self.search_scholar_labs_fast(current_tab, query)
                
                if success and i < len(queries) - 1:
                    # ë‹¤ìŒ ì§ˆë¬¸ì´ ìˆìœ¼ë©´, í˜„ì¬ íƒ­ì˜ ì¸ìš© ì¶”ì¶œì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì˜ˆì•½
                    print(f"    ğŸ”„ ë‹¤ìŒ ì§ˆë¬¸ ì§„í–‰ (í˜„ì¬ íƒ­ ì¸ìš©ì€ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)")
                    extraction_task = asyncio.create_task(
                        self.extract_citations_from_page(current_tab, max_results=10)
                    )
                    prev_tab = current_tab
                    prev_query = query
                else:
                    # ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°: ì¦‰ì‹œ ì¶”ì¶œ
                    print(f"    ğŸ“¥ ì¸ìš© ì •ë³´ ì¶”ì¶œ ì¤‘...")
                    results = await self.extract_citations_from_page(current_tab, max_results=10)
                    batch_results.append((query, results))
                    print(f"    âœ… {len(results)}ê°œ ì™„ë£Œ")
                    await current_tab.close()
                
                # ì„¸ì…˜ í•œë„ ì²´í¬
                if await self._check_session_limit(page):
                    print("\nâš ï¸  ì„¸ì…˜ í•œë„ ë„ë‹¬")
                    if extraction_task:
                        prev_results = await extraction_task
                        batch_results.append((prev_query, prev_results))
                        await prev_tab.close()
                    break
            
            # ë‚¨ì€ ì¶”ì¶œ ì‘ì—… ì™„ë£Œ
            if extraction_task:
                print(f"\nâ³ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¸ìš© ì¶”ì¶œ ì™„ë£Œ ëŒ€ê¸°...")
                prev_results = await extraction_task
                batch_results.append((prev_query, prev_results))
                await prev_tab.close()
            
            await context.close()
            print(f"\nğŸ”’ ì„¸ì…˜ ì¢…ë£Œ (ì´ {len(batch_results)}ê°œ ì™„ë£Œ)")
        
        return batch_results
    
    async def _check_login_required(self, page) -> bool:
        """ë¡œê·¸ì¸ì´ í•„ìš”í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # Google ë¡œê·¸ì¸ í˜ì´ì§€ í™•ì¸
            if 'accounts.google.com' in page.url:
                return True
            # ë¡œê·¸ì¸ ë²„íŠ¼ í™•ì¸
            login_btn = await page.query_selector('a[href*="accounts.google.com"]')
            return login_btn is not None
        except:
            return False
    
    async def _check_daily_limit(self, page) -> bool:
        """ì¼ì¼ í•œë„ ë„ë‹¬ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            page_text = await page.inner_text('body')
            limit_keywords = ['ì¼ì¼ í•œë„', 'daily limit', 'í•œë„ì— ë„ë‹¬', 'limit reached']
            return any(kw in page_text.lower() for kw in limit_keywords)
        except:
            return False
    
    async def _check_session_limit(self, page) -> bool:
        """ì„¸ì…˜ í•œë„ ë„ë‹¬ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            page_text = await page.inner_text('body')
            session_keywords = ['ì„¸ì…˜ í•œë„', 'session limit', 'ì„¸ì…˜ì´ ë§Œë£Œ']
            return any(kw in page_text.lower() for kw in session_keywords)
        except:
            return False
    
    def format_results_markdown(self, all_query_results: list[tuple[str, list[dict]]], 
                                 start_q_num: int = 1, 
                                 append_mode: bool = False) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ í¬ë§·í•©ë‹ˆë‹¤."""
        lines = []
        
        if not append_mode:
            lines = [
                "# Google Scholar Labs ì‹œë§¨í‹± ê²€ìƒ‰ ê²°ê³¼",
                "",
                f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                "",
                "---",
                ""
            ]
        
        for i, (query, results) in enumerate(all_query_results):
            q_num = start_q_num + i
            lines.append(f"### Q{q_num}: {query}")
            lines.append("")
            
            if not results:
                lines.append("*ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.*")
                lines.append("")
                continue
            
            for j, r in enumerate(results, 1):
                # 1. ë²ˆí˜¸ì™€ ì œëª© (ë§í¬ í¬í•¨)
                title = r.get('title', 'Untitled')
                link = r.get('link', '')
                
                if link:
                    lines.append(f"**{j}. [{title}]({link})**")
                else:
                    lines.append(f"**{j}. {title}**")
                lines.append("")
                
                # 2. ë©”íƒ€ ì •ë³´ (ì €ì - ì €ë„, ì—°ë„ - ì¶œíŒì‚¬)
                if r.get('meta'):
                    lines.append(f"{r['meta']}")
                    lines.append("")
                
                # 3. ì£¼ìš” ì„¤ëª… (ì²« ë²ˆì§¸ ìŠ¤ë‹ˆí« ë¬¸ì¥)
                if r.get('snippet'):
                    snippet = r['snippet'].replace('\n', ' ').strip()
                    # ì²« ë¬¸ì¥ì€ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ
                    sentences = re.split(r'(?<=[.!?])\s+', snippet)
                    if sentences:
                        lines.append(sentences[0])
                        lines.append("")
                        # ë‚˜ë¨¸ì§€ ë¬¸ì¥ë“¤ì€ bullet pointë¡œ
                        for sent in sentences[1:]:
                            if sent.strip():
                                lines.append(f"â€¢ {sent.strip()}")
                        if len(sentences) > 1:
                            lines.append("")
                
                # 4. ì¸ìš© ìˆ˜
                if r.get('citations'):
                    lines.append(f"ğŸ“š {r['citations']}")
                    lines.append("")
                
                # 5. MLA ì¸ìš© í˜•ì‹ (íŒŒì´í”„ë¼ì¸ í˜¸í™˜: MLAë§Œ ì¶œë ¥)
                if r.get('mla_citation'):
                    lines.append(f"> {r['mla_citation']}")
                    lines.append("")
                
                lines.append("---")
                lines.append("")
            
            lines.append("")
        
        return '\n'.join(lines)
    
    def _generate_citation(self, result: dict) -> str:
        """í•™ìˆ  ì¸ìš© í˜•ì‹ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        meta = result.get('meta', '')
        title = result.get('title', '')
        
        if not meta:
            return ""
        
        # ë©”íƒ€ ì •ë³´ íŒŒì‹±: "ì €ì - ì €ë„, ì—°ë„ - ì¶œíŒì‚¬" í˜•ì‹
        # ì˜ˆ: "TD Stegman - Theological studies, 2011 - journals.sagepub.com"
        
        parts = meta.split(' - ')
        
        if len(parts) >= 2:
            authors_raw = parts[0].strip()
            
            # ì €ì ì´ë¦„ ì •ë¦¬ (ì´ë‹ˆì…œ + ì„± -> ì„±, ì´ë‹ˆì…œ)
            author_parts = authors_raw.split(', ')
            authors = []
            for author in author_parts[:3]:  # ìµœëŒ€ 3ëª…
                author = author.strip()
                if author:
                    # "TD Stegman" -> "Stegman, T. D." í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì‹œë„
                    words = author.split()
                    if len(words) >= 2:
                        initials = words[:-1]
                        surname = words[-1]
                        formatted_initials = '. '.join(list(''.join(initials))) + '.'
                        authors.append(f"{surname}, {formatted_initials}")
                    else:
                        authors.append(author)
            
            author_str = ', '.join(authors)
            if len(author_parts) > 3:
                author_str += ", et al."
            
            # ì €ë„ê³¼ ì—°ë„ ì¶”ì¶œ
            journal_year = parts[1] if len(parts) > 1 else ""
            year_match = re.search(r'(\d{4})', journal_year)
            year = year_match.group(1) if year_match else ""
            journal = re.sub(r',?\s*\d{4}', '', journal_year).strip()
            
            # ì¸ìš© í˜•ì‹ ì¡°í•©
            if author_str and title and journal and year:
                return f'{author_str} "{title}." {journal} ({year}).'
            elif author_str and title and year:
                return f'{author_str} "{title}." ({year}).'
        
        return ""
    
    def _clean_citations_mla_only(self, markdown: str) -> str:
        """APA, ISO 690 ì¸ìš©ì„ ì œê±°í•˜ê³  MLA ì¸ìš©ë§Œ ë‚¨ê¹ë‹ˆë‹¤."""
        lines = markdown.split('\n')
        cleaned_lines = []
        skip_next_empty = False
        
        for i, line in enumerate(lines):
            # APA í˜•ì‹ ê°ì§€: ì €ì (ì—°ë„). ì œëª©.
            # ISO 690 í˜•ì‹ ê°ì§€: ì €ì. ì œëª©. ì €ë„, ì—°ë„, ê¶Œ(í˜¸): í˜ì´ì§€
            if line.startswith('>'):
                content = line[1:].strip()
                # APA íŒ¨í„´: "ì €ì, A. B. (2020)." ë˜ëŠ” "(2011)." í¬í•¨
                if re.search(r'\(\d{4}\)\.', content):
                    skip_next_empty = True
                    continue
                # ISO 690 íŒ¨í„´: "ì €ì. ì œëª©. ì €ë„, ì—°ë„, ê¶Œ.í˜¸: í˜ì´ì§€" (ì½œë¡ +ìˆ«ì íŒ¨í„´)
                elif re.search(r'\d+\.\d+:\s*\d+-\d+', content):
                    skip_next_empty = True
                    continue
            
            # ë¹ˆ ì¤„ ìŠ¤í‚µ ì²˜ë¦¬
            if skip_next_empty and line.strip() == '':
                skip_next_empty = False
                continue
            
            skip_next_empty = False
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def save_results(self, markdown: str, append: bool = False):
        """ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥ (ëˆ„ì  ë˜ëŠ” ìƒˆë¡œ ìƒì„±, MLA ì¸ìš©ë§Œ ìœ ì§€)"""
        if append and self.output_file.exists():
            existing = self.output_file.read_text(encoding='utf-8')
            markdown = existing + "\n" + markdown
        
        # MLA ì¸ìš©ë§Œ ë‚¨ê¸°ê³  APA, ISO 690 ì œê±°
        markdown = self._clean_citations_mla_only(markdown)
        
        self.output_file.write_text(markdown, encoding='utf-8')
        return self.output_file
    
    async def run(self, queries_input: str = None, resume: bool = False):
        """ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("=" * 60)
        print("Google Scholar Labs ì‹œë§¨í‹± ê²€ìƒ‰ ì—ì´ì „íŠ¸ v2")
        print(f"  â€¢ ë°°ì¹˜ í¬ê¸°: {self.BATCH_SIZE}ê°œ/ì„¸ì…˜")
        print(f"  â€¢ ì„¸ì…˜ ì œí•œ ëŒ€ì‘: ìë™ ë¸Œë¼ìš°ì € ì¬ì‹œì‘")
        print("=" * 60)
        
        # ì´ì „ ìƒíƒœ í™•ì¸
        state = self.load_state()
        
        if resume and state["completed_queries"]:
            print(f"\nğŸ“‚ ì´ì „ ê²€ìƒ‰ ì¬ê°œ: {len(state['completed_queries'])}ê°œ ì™„ë£Œë¨")
        else:
            state = {"completed_queries": [], "results": []}
        
        # ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
        if queries_input is None:
            print("\nê²€ìƒ‰ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ 2ë²ˆìœ¼ë¡œ ì…ë ¥ ì™„ë£Œ):")
            print("-" * 40)
            lines = []
            empty_count = 0
            
            while True:
                try:
                    line = input()
                    if line == "":
                        empty_count += 1
                        if empty_count >= 2:
                            break
                    else:
                        empty_count = 0
                    lines.append(line)
                except EOFError:
                    break
            
            queries_input = '\n'.join(lines)
        
        # ì§ˆë¬¸ íŒŒì‹±
        all_queries = self.parse_queries(queries_input)
        
        if not all_queries:
            print("ìœ íš¨í•œ ê²€ìƒ‰ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì´ë¯¸ ì™„ë£Œëœ ì§ˆë¬¸ ì œì™¸
        remaining_queries = [q for q in all_queries if q not in state["completed_queries"]]
        
        print(f"\nğŸ“‹ ì´ ì§ˆë¬¸: {len(all_queries)}ê°œ")
        print(f"   ì™„ë£Œë¨: {len(state['completed_queries'])}ê°œ")
        print(f"   ë‚¨ì€ ì§ˆë¬¸: {len(remaining_queries)}ê°œ")
        
        if not remaining_queries:
            print("\nëª¨ë“  ì§ˆë¬¸ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        batches = [remaining_queries[i:i + self.BATCH_SIZE] 
                   for i in range(0, len(remaining_queries), self.BATCH_SIZE)]
        
        print(f"   ë°°ì¹˜ ìˆ˜: {len(batches)}ê°œ ({self.BATCH_SIZE}ê°œ/ë°°ì¹˜)")
        
        # ê° ë°°ì¹˜ ì‹¤í–‰
        all_batch_results = []
        processed_count = len(state["completed_queries"])
        
        for batch_idx, batch in enumerate(batches):
            print(f"\n{'='*40}")
            print(f"ğŸ“¦ ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ì‹¤í–‰ ì¤‘...")
            print(f"{'='*40}")
            
            batch_results = await self.run_batch(batch, processed_count)
            all_batch_results.extend(batch_results)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì €ì¥
            for query, results in batch_results:
                state["completed_queries"].append(query)
                state["results"].append({"query": query, "results": results})
            
            self.save_state(state)
            
            # ê²°ê³¼ ëˆ„ì  ì €ì¥
            is_first_batch = (batch_idx == 0 and processed_count == 0)
            markdown = self.format_results_markdown(
                batch_results, 
                start_q_num=processed_count + 1,
                append_mode=not is_first_batch
            )
            output_file = self.save_results(markdown, append=not is_first_batch)
            
            processed_count += len(batch)
            
            print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {output_file}")
            
            # ë‹¤ìŒ ë°°ì¹˜ ì „ ëŒ€ê¸° (ì„¸ì…˜ ì¿¨ë‹¤ìš´)
            if batch_idx < len(batches) - 1:
                cooldown = 10
                print(f"\nâ³ ì„¸ì…˜ ì¿¨ë‹¤ìš´ ëŒ€ê¸° ì¤‘... ({cooldown}ì´ˆ)")
                await asyncio.sleep(cooldown)
        
        # ì™„ë£Œ
        self.clear_state()  # ìƒíƒœ íŒŒì¼ ì •ë¦¬
        
        print()
        print("=" * 60)
        print(f"âœ… ì™„ë£Œ! ì´ {len(all_queries)}ê°œ ì§ˆë¬¸ ê²€ìƒ‰ë¨")
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {output_file}")
        print("=" * 60)


async def main():
    # ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬
    profile = "default"
    job = None  # Noneì´ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ìë™ ìƒì„±
    resume = "--resume" in sys.argv
    
    # --profile ì˜µì…˜ ì²˜ë¦¬
    for i, arg in enumerate(sys.argv[1:], 1):
        if arg == "--profile" and i < len(sys.argv) - 1:
            profile = sys.argv[i + 1]
        elif arg.startswith("--profile="):
            profile = arg.split("=")[1]
    
    # --job ì˜µì…˜ ì²˜ë¦¬
    for i, arg in enumerate(sys.argv[1:], 1):
        if arg == "--job" and i < len(sys.argv) - 1:
            job = sys.argv[i + 1]
        elif arg.startswith("--job="):
            job = arg.split("=")[1]
    
    # --list-profiles: ë“±ë¡ëœ í”„ë¡œí•„ ëª©ë¡ ë³´ê¸°
    if "--list-profiles" in sys.argv:
        profiles_dir = Path(__file__).parent / ".profiles"
        if profiles_dir.exists():
            profiles = [p.name for p in profiles_dir.iterdir() if p.is_dir()]
            print("ğŸ“ ë“±ë¡ëœ í”„ë¡œí•„:")
            for p in profiles:
                print(f"   â€¢ {p}")
        else:
            print("ë“±ë¡ëœ í”„ë¡œí•„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # --list-jobs: ì§„í–‰ ì¤‘ì¸ ì‘ì—… ëª©ë¡ ë³´ê¸°
    if "--list-jobs" in sys.argv:
        results_dir = Path(__file__).parent / "results"
        if results_dir.exists():
            states = list(results_dir.glob(".state_*.json"))
            if states:
                print("ğŸ“‹ ì§„í–‰ ì¤‘ì¸ ì‘ì—…:")
                for s in states:
                    job_name = s.stem.replace(".state_", "")
                    print(f"   â€¢ {job_name}")
            else:
                print("ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    agent = ScholarLabsAgent(profile=profile, job=job)
    
    # íŒŒì¼ì—ì„œ ì§ˆë¬¸ ì½ê¸°
    for arg in sys.argv[1:]:
        if arg.startswith("--"):
            continue
        queries_file = Path(arg)
        if queries_file.exists():
            queries_input = queries_file.read_text(encoding='utf-8')
            await agent.run(queries_input, resume=resume)
            return
    
    # ëŒ€í™”í˜• ëª¨ë“œ
    await agent.run(resume=resume)


if __name__ == "__main__":
    asyncio.run(main())


