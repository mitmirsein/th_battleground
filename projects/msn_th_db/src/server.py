"""
MCP Server for msn_th_db - Theology Archive

A stateless MCP server providing keyword-based search over JSONL archive.
Semantic filtering is delegated to Antigravity (LLM).

Tools:
    - search: 3-language expanded keyword search
    - get_chunk: Retrieve full chunk by global_chunk_id
    - list_sources: List available sources in archive
"""

import asyncio
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import TextContent, Tool

from models import SearchResponse, SourceInfo
from searcher import ArchiveSearcher
from translator import glossary_manager, translation_archive

# Configuration
ARCHIVE_PATH = Path.home() / "Desktop" / "MS_Dev.nosync" / "data" / "msn_th_archive"
GLOSSARY_PATH = Path(__file__).parent.parent / "config" / "glossary.json"

# Initialize server and searcher
app = Server("msn_th_db")
searcher = ArchiveSearcher(ARCHIVE_PATH, GLOSSARY_PATH)


@app.list_tools()
async def list_tools() -> list[Tool]:
    """List available tools."""
    return [
        Tool(
            name="search",
            description="""ì‹ í•™ ì•„ì¹´ì´ë¸Œì—ì„œ 3ì¤‘ ì–¸ì–´(í•œêµ­ì–´/ì˜ì–´/ë…ì¼ì–´) í™•ì¥ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            
í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë©°, ì‹œë§¨í‹± í•„í„°ë§ì€ Antigravityê°€ ë‹´ë‹¹í•©ë‹ˆë‹¤.
ê²€ìƒ‰ ê²°ê³¼ì—ëŠ” snippet, citation, match_termsê°€ í¬í•¨ë©ë‹ˆë‹¤.

ì˜ˆì‹œ: query="ì¹­ì˜" â†’ [ì¹­ì˜, Justification, Rechtfertigung]ìœ¼ë¡œ í™•ì¥ ê²€ìƒ‰""",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "ê²€ìƒ‰ì–´ (í•œêµ­ì–´/ì˜ì–´/ë…ì¼ì–´)"
                    },
                    "languages": {
                        "type": "array",
                        "items": {"type": "string"},
                        "default": ["ko", "en", "de"],
                        "description": "í™•ì¥ ê²€ìƒ‰ ëŒ€ìƒ ì–¸ì–´"
                    },
                    "source": {
                        "type": "string",
                        "description": "ì†ŒìŠ¤ í•„í„° (doc_id ì ‘ë‘ì‚¬, ì˜ˆ: RGG, TRE, EKL)"
                    },
                    "limit": {
                        "type": "integer",
                        "default": 10,
                        "description": "ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ 10)"
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="get_chunk",
            description="""global_chunk_idë¡œ íŠ¹ì • ì²­í¬ì˜ ì „ì²´ ë‚´ìš©ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

ì²­í¬ ID í˜•ì‹: {doc_id}:{chunk_id} (ì˜ˆ: RGG_4_4:0235_001)
ë°˜í™˜ê°’ì—ëŠ” ì „ì²´ í…ìŠ¤íŠ¸, citation, ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ë©ë‹ˆë‹¤.""",
            inputSchema={
                "type": "object",
                "properties": {
                    "global_chunk_id": {
                        "type": "string",
                        "description": "ì „ì—­ ì²­í¬ ID (ì˜ˆ: RGG_4_4:0235_001)"
                    }
                },
                "required": ["global_chunk_id"]
            }
        ),
        Tool(
            name="list_sources",
            description="""ì•„ì¹´ì´ë¸Œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì†ŒìŠ¤(ë¬¸ì„œ) ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

ê° ì†ŒìŠ¤ì— ëŒ€í•´ doc_id, ì œëª©, ì–¸ì–´, ë¬¸ì„œ ìœ í˜•, ì´ ì²­í¬ ìˆ˜ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.""",
            inputSchema={
                "type": "object",
                "properties": {}
            }
        ),
        Tool(
            name="lookup_term",
            description="""Glossary(v2.0) ìš©ì–´ ì¡°íšŒ. TRE Lemma ê¸°ë°˜ì˜ ì‹ í•™ ìš©ì–´ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            
canonical(í‘œì¤€ ë²ˆì—­), definition(ì •ì˜) ë“±ì„ ë°˜í™˜í•©ë‹ˆë‹¤.""",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "ì¡°íšŒí•  ìš©ì–´ (í•œêµ­ì–´/ë…ì¼ì–´/ì˜ì–´)"
                    },
                    "lang": {
                        "type": "string",
                        "default": "de",
                        "description": "ëŒ€ìƒ ì–¸ì–´ (ê¸°ë³¸: de)"
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="save_translation",
            description="""ë²ˆì—­ëœ ì²­í¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
            
JSONL í¬ë§·ìœ¼ë¡œ _KR.jsonl íŒŒì¼ì— ì•„ì¹´ì´ë¹™í•©ë‹ˆë‹¤.""",
            inputSchema={
                "type": "object",
                "properties": {
                    "doc_id": {
                        "type": "string",
                        "description": "ë¬¸ì„œ ID"
                    },
                    "chunk_id": {
                        "type": "string",
                        "description": "ì²­í¬ ID"
                    },
                    "original_text": {
                        "type": "string",
                        "description": "ì›ë³¸ í…ìŠ¤íŠ¸"
                    },
                    "translated_text": {
                        "type": "string",
                        "description": "ë²ˆì—­ëœ í…ìŠ¤íŠ¸"
                    }
                },
                "required": ["doc_id", "chunk_id", "original_text", "translated_text"]
            }
        ),
        Tool(
            name="fetch_translation_batch",
            description="ë²ˆì—­í•  ì²­í¬ ë°°ì¹˜(ê¸°ë³¸ 5ê°œ)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. status='todo'ì¸ í•­ëª©ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.",
            inputSchema={
                "type": "object",
                "properties": {
                    "doc_id": {"type": "string"},
                    "size": {"type": "integer", "default": 5}
                },
                "required": ["doc_id"]
            }
        ),
        Tool(
            name="submit_translation_batch",
            description="ë²ˆì—­ëœ ì²­í¬ ë°°ì¹˜ë¥¼ ì¼ê´„ ì €ì¥í•©ë‹ˆë‹¤.",
            inputSchema={
                "type": "object",
                "properties": {
                    "doc_id": {"type": "string"},
                    "translations": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "chunk_id": {"type": "string"},
                                "translation": {"type": "string"}
                            },
                            "required": ["chunk_id", "translation"]
                        }
                    }
                },
                "required": ["doc_id", "translations"]
            }
        )
    ]


@app.call_tool()
async def call_tool(name: str, arguments: dict) -> list[TextContent]:
    """Handle tool calls."""
    
    if name == "search":
        query = arguments.get("query", "")
        languages = arguments.get("languages", ["ko", "en", "de"])
        source = arguments.get("source")
        limit = arguments.get("limit", 10)
        
        response = searcher.search(
            query=query,
            languages=languages,
            source=source,
            limit=limit
        )
        
        # Format response
        result_text = f"## ê²€ìƒ‰ ê²°ê³¼\n\n"
        result_text += f"**ì¿¼ë¦¬**: {query}\n"
        result_text += f"**í™•ì¥ ê²€ìƒ‰ì–´**: {', '.join(response.expanded_queries)}\n"
        result_text += f"**ì´ ê²°ê³¼**: {response.total_matches}ê°œ\n\n"
        
        for i, r in enumerate(response.results, 1):
            result_text += f"### [{i}] {r.citation}\n"
            result_text += f"- **ID**: `{r.global_chunk_id}`\n"
            result_text += f"- **í˜ì´ì§€**: {r.printed_page}\n"
            result_text += f"- **ë§¤ì¹­**: {', '.join(r.match_terms)} ({r.match_count}íšŒ)\n"
            result_text += f"- **ë°œì·Œ**:\n> {r.snippet}\n\n"
        
        return [TextContent(type="text", text=result_text)]
    
    elif name == "get_chunk":
        global_chunk_id = arguments.get("global_chunk_id", "")
        
        chunk = searcher.get_chunk(global_chunk_id)
        
        if not chunk:
            return [TextContent(
                type="text",
                text=f"âŒ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: `{global_chunk_id}`"
            )]
        
        result_text = f"## ì²­í¬ ìƒì„¸\n\n"
        result_text += f"**ID**: `{chunk['global_chunk_id']}`\n"
        result_text += f"**ë¬¸ì„œ**: {chunk['doc_id']}\n"
        result_text += f"**PDF í˜ì´ì§€**: {chunk['pdf_page']}\n"
        result_text += f"**ì¸ì‡„ í˜ì´ì§€**: {chunk['printed_page']}\n"
        result_text += f"**ì¸ìš©**: {chunk['citation']}\n\n"
        result_text += f"### ë‚´ìš©\n\n{chunk['content']}\n"
        
        if chunk.get('themes'):
            result_text += f"\n### í…Œë§ˆ\n{', '.join(chunk['themes'])}\n"
        
        return [TextContent(type="text", text=result_text)]
    
    elif name == "list_sources":
        sources = searcher.list_sources()
        
        if not sources:
            return [TextContent(
                type="text",
                text="ğŸ“­ ì•„ì¹´ì´ë¸Œì— ì†ŒìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n\nì²­í‚¹ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
            )]
        
        result_text = f"## ì‚¬ìš© ê°€ëŠ¥í•œ ì†ŒìŠ¤\n\n"
        result_text += f"ì´ **{len(sources)}**ê°œ ë¬¸ì„œ\n\n"
        
        for s in sources:
            result_text += f"### {s.abbr}"
            if s.volume:
                result_text += f" Vol. {s.volume}"
            if s.edition:
                result_text += f" ({s.edition}íŒ)"
            result_text += "\n"
            result_text += f"- **ID**: `{s.doc_id}`\n"
            result_text += f"- **ì œëª©**: {s.title}\n"
            result_text += f"- **ì–¸ì–´**: {s.language}\n"
            result_text += f"- **ìœ í˜•**: {s.doc_type}\n"
            result_text += f"- **ì²­í¬ ìˆ˜**: {s.total_chunks}\n\n"
        
        return [TextContent(type="text", text=result_text)]
    
    elif name == "lookup_term":
        query = arguments.get("query", "")
        lang = arguments.get("lang", "de")
        
        results = glossary_manager.lookup(query, lang)
        
        if not results:
             return [TextContent(type="text", text=f"ğŸ” ìš©ì–´ '{query}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]
             
        # Format output (take top 3 matching)
        out = f"## ìš©ì–´ ì¡°íšŒ: {query}\n\n"
        for idx, item in enumerate(results[:3]):
            canonical = item.get("canonical", {})
            out += f"### {canonical.get('de', query)}\n"
            out += f"- **KO**: {canonical.get('ko', '-')}\n"
            out += f"- **EN**: {canonical.get('en', '-')}\n"
            out += f"- **FR**: {canonical.get('fr', '-')}\n"
            
            definitions = item.get("definitions", {})
            if "ko" in definitions:
                 out += f"- **ì •ì˜(KO)**: {definitions['ko']}\n"
            
            out += "\n"
            
        return [TextContent(type="text", text=out)]
        
    elif name == "save_translation":
        doc_id = arguments.get("doc_id")
        chunk_id = arguments.get("chunk_id")
        original = arguments.get("original_text")
        translated = arguments.get("translated_text")
        
        path = translation_archive.save_translation(doc_id, chunk_id, original, translated)
        
        return [TextContent(type="text", text=f"âœ… ë²ˆì—­ ì €ì¥ ì™„ë£Œ: `{path}`")]
        
    elif name == "fetch_translation_batch":
        doc_id = arguments.get("doc_id")
        size = arguments.get("size", 5)
        
        batch = translation_archive.get_next_batch(doc_id, size)
        
        if not batch:
            return [TextContent(type="text", text="ğŸ‰ ëª¨ë“  ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤)")]
            
        # Serialize batch to JSON string for the agent to parse
        import json
        return [TextContent(type="text", text=json.dumps(batch, ensure_ascii=False, indent=2))]
        
    elif name == "submit_translation_batch":
        doc_id = arguments.get("doc_id")
        translations = arguments.get("translations", [])
        
        count = translation_archive.save_batch(doc_id, translations)
        
        return [TextContent(type="text", text=f"âœ… {count}ê°œ ì²­í¬ ì¼ê´„ ì €ì¥ ì™„ë£Œ.")]
    
    else:
        return [TextContent(
            type="text",
            text=f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {name}"
        )]


async def main():
    """Run the MCP server."""
    async with stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            app.create_initialization_options()
        )


if __name__ == "__main__":
    asyncio.run(main())
