import os
import json
import subprocess
import sys

def search_archive(query, data_dir="data"):
    """
    Simulates the 'Grep Retrieval' process.
    It looks for specific keywords in the JSONL files.
    """
    print(f"ğŸ” Searching for keywords: '{query}' in {data_dir}...")
    
    # Simple grep (case insensitive) simulating the engine
    # In real world: rg -i "keyword" data/
    
    found_chunks = []
    
    # Keywords strategy: simple split
    keywords = query.split()
    
    for root, dirs, files in os.walk(data_dir):
        for file in files:
            if file.endswith(".jsonl"):
                path = os.path.join(root, file)
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        # ALL keywords must be present for a 'strict' match, 
                        # or ANY for a 'loose' match. Let's use ANY for demo.
                        if any(k in line for k in keywords):
                            try:
                                doc = json.loads(line)
                                found_chunks.append(doc)
                            except:
                                pass
                                
    return found_chunks

def generate_response(query, context_chunks):
    """
    Simulates the 'LLM Generation' part.
    Since this is a script, we will use a simple rule-based template 
    to demonstrate how the 'Found Context' is used.
    """
    print("\nğŸ¤– [AI Agent Logic] Generating Response based on Evidence...")
    
    if not context_chunks:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ê·œì •ì´ë‚˜ ê³¼ê±° ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    response = []
    response.append(f"â“ ì§ˆë¬¸: {query}")
    response.append("\nâœ… ë¶„ì„ ê²°ê³¼ ë° ëŒ€ì‘ ë°©ì•ˆ:")
    
    # Categorize findings
    sops = [c for c in context_chunks if "SOP" in c.get('doc_id', '') or "REG" in c.get('doc_id','')]
    devs = [c for c in context_chunks if "DEV" in c.get('doc_id', '')]
    
    if sops:
        response.append("\n[1. ê´€ë ¨ ê·œì • ë° SOP]")
        for i, doc in enumerate(sops, 1):
            response.append(f"  - ({doc['doc_id']}) {doc.get('content')[:100]}...")
            
    if devs:
        response.append("\n[2. ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€ (Lesson Learned)]")
        for i, doc in enumerate(devs, 1):
            response.append(f"  - ({doc['doc_id']}) {doc.get('content')}")
            
    response.append("\nğŸ’¡ [ê¶Œì¥ ì¡°ì¹˜ (Action Item)]")
    if "ì ì°©ë ¥" in query:
        response.append("  1. SOP-QA-001ì— ì˜ê±°, ì¦‰ì‹œ ìƒì‚°ì„ ì¤‘ë‹¨í•˜ê³  ê²©ë¦¬(Quarantine) ì¡°ì¹˜í•˜ì‹­ì‹œì˜¤.")
        response.append("  2. SOP-MF-012ì— ì˜ê±°, ê±´ì¡°ê¸° ì˜¨ë„ê°€ 80â„ƒ ë¯¸ë§Œì¸ì§€ ì ê²€í•˜ì‹­ì‹œì˜¤.")
        response.append("  3. ê³¼ê±° ì‚¬ë¡€(DEV-230205)ì™€ ë™ì¼í•˜ê²Œ íˆí„° ê³ ì¥ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
        
    return "\n".join(response)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python agent_sim.py 'search query'")
        sys.exit(1)
        
    query = sys.argv[1]
    
    # 1. Retrieve
    chunks = search_archive(query)
    
    # 2. Re-rank (Skipped for demo, simple list)
    print(f"ğŸ“Š Found {len(chunks)} relevant documents.\n")
    
    # 3. Generate
    final_output = generate_response(query, chunks)
    print(final_output)
